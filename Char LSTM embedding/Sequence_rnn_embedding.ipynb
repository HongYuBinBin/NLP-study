{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sequence_rnn_embedding.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMYYB4q16cJQeetpGe6yXKT"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yQLQ1_awighc"},"source":["# 2. Sequence 단위 RNN(Char RNN)"]},{"cell_type":"code","metadata":{"id":"yqTPYKksifwk","executionInfo":{"status":"ok","timestamp":1611822742341,"user_tz":-540,"elapsed":3231,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}}},"source":["import torch\r\n","import torch.nn as nn\r\n","import torch.optim as optim\r\n","\r\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nnv5TotJileQ"},"source":["## 1. 훈련 데이터 전처리하기"]},{"cell_type":"code","metadata":{"id":"SHJ3pg0YfgRo","executionInfo":{"status":"ok","timestamp":1611822742345,"user_tz":-540,"elapsed":3227,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}}},"source":["#sentence = (\"if you want to build a ship, don't drum up people together to \"\r\n","#            \"collect wood and don't assign them tasks and work, but rather \"\r\n","#            \"teach them to long for the endless immensity of the sea.\")"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZB67BbKdTqka","executionInfo":{"status":"ok","timestamp":1611822742346,"user_tz":-540,"elapsed":3219,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}}},"source":["sentence = \"Repeat is the best medicine for memory\".split()"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eJutPXqNevgK","executionInfo":{"status":"ok","timestamp":1611822742347,"user_tz":-540,"elapsed":3201,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}},"outputId":"3c79351a-0e65-44bc-90b4-05c9bfdf9de4"},"source":["vocab = list(set(sentence))\r\n","print(vocab)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["['is', 'the', 'medicine', 'best', 'memory', 'Repeat', 'for']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xrVvjptOfgTw","executionInfo":{"status":"ok","timestamp":1611822742347,"user_tz":-540,"elapsed":3197,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}}},"source":["word2index = {tkn: i for i, tkn in enumerate(vocab, 1)}  # 단어에 고유한 정수 부여\r\n","word2index['<unk>']=0"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C2Im2zgwfgV_","executionInfo":{"status":"ok","timestamp":1611822742348,"user_tz":-540,"elapsed":3188,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}},"outputId":"5df2209a-d7cd-4880-ef44-7f6cf34a19ff"},"source":["print(word2index)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["{'is': 1, 'the': 2, 'medicine': 3, 'best': 4, 'memory': 5, 'Repeat': 6, 'for': 7, '<unk>': 0}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_SjLE0mIe-Pl","executionInfo":{"status":"ok","timestamp":1611822742348,"user_tz":-540,"elapsed":3180,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}},"outputId":"a62cce67-d4ab-4bbe-f0bc-7b44464c5c6b"},"source":["print(word2index['memory'])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zl8zs8maT8Oe","executionInfo":{"status":"ok","timestamp":1611822742349,"user_tz":-540,"elapsed":3171,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}},"outputId":"75b252b0-2e17-4023-a9d5-aa489a55a942"},"source":["index2word = {v: k for k, v in word2index.items()}\r\n","print(index2word)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["{1: 'is', 2: 'the', 3: 'medicine', 4: 'best', 5: 'memory', 6: 'Repeat', 7: 'for', 0: '<unk>'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mm0dkvxSfDgw","executionInfo":{"status":"ok","timestamp":1611822742349,"user_tz":-540,"elapsed":3163,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}},"outputId":"6a774cb1-1126-4045-fa1d-dd84cf163490"},"source":["print(index2word[2])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["the\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lEI2VMl_UKjM","executionInfo":{"status":"ok","timestamp":1611822742350,"user_tz":-540,"elapsed":2738,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}}},"source":["# Char data와 다르게 input과 output을 하나로 구성\r\n","# => Sequence data이기 때문이다\r\n","\r\n","def build_data(sequence, word2index):\r\n","  encoded = [word2index[token] for token in sentence]  # 각 문자를 정수로 변환\r\n","  input_seq, label_seq = encoded[:-1], encoded[1:]  # 입력 시퀀스와 레이블 시퀀스를 분리\r\n","  input_seq = torch.LongTensor(input_seq).unsqueeze(0)  # 배치 차원추가(batch size를 위한)  # Longtensor? char 단위일때는 flaot/ sequence 단위일 때는 Long?\r\n","  label_seq = torch.LongTensor(label_seq).unsqueeze(0)  # 배치 차원 추가\r\n","\r\n","  return input_seq, label_seq"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"DrmmQIB4Uv2z","executionInfo":{"status":"ok","timestamp":1611822745433,"user_tz":-540,"elapsed":796,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}}},"source":["X, Y = build_data(sentence, word2index)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ytTwtClJUs5V","executionInfo":{"status":"ok","timestamp":1611822745856,"user_tz":-540,"elapsed":852,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}},"outputId":"5d8985e9-5614-4d70-ce07-754ec67f7a83"},"source":["print(X)\r\n","print(Y)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["tensor([[6, 1, 2, 4, 3, 7]])\n","tensor([[1, 2, 4, 3, 7, 5]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TUPfrOJvqCCt","executionInfo":{"status":"ok","timestamp":1611663947546,"user_tz":-540,"elapsed":658,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}},"outputId":"79a6251e-1c47-465d-9258-8de43ba837cd"},"source":["len(word2index) "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"PvXIzjgTibqY"},"source":["# 2. 모델구현하기"]},{"cell_type":"markdown","metadata":{"id":"cofPM6Z8zYAj"},"source":["https://dgkim5360.tistory.com/entry/deep-learning-for-nlp-with-pytorch-tutorial-part-2"]},{"cell_type":"code","metadata":{"id":"8DhbfbUxf6PV"},"source":["# 시퀀스 길이 = 6 / 7 => 6 앞 뒤에서 하나씩 뺐기 때문이다 / encoded[:-1], encoded[1:]\r\n","# 임베딩 차원 = 5 / 임베딩 차원이 5개라는 것은 관계지을 수 있는 단어의 개수가 5개라는 뜻이다\r\n","# 은닉층 크기 = 5 / 임베딩 차원과 동일하게?\r\n","# vocab_size = 8 \r\n","\r\n","# [5, 4, 1, 6, 2, 3]\r\n","\r\n","class Net(nn.Module):\r\n","    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\r\n","        super(Net, self).__init__()\r\n","        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, # 워드 임베딩 / vocab_size = real input size\r\n","                                            embedding_dim=input_size)\r\n","        self.rnn_layer = nn.RNN(input_size, hidden_size, # 입력 차원, 은닉 상태의 크기 정의\r\n","                                batch_first=batch_first)\r\n","        self.linear = nn.Linear(hidden_size, vocab_size) # 출력은 원-핫 벡터의 크기를 가져야함. 또는 단어 집합의 크기만큼 가져야함.\r\n","\r\n","    def forward(self, x):\r\n","        # 1. 임베딩 층\r\n","        # 크기변화: (배치 크기, 시퀀스 길이) => (배치 크기, 시퀀스 길이, 임베딩 차원)\r\n","        output = self.embedding_layer(x)\r\n","        # 2. RNN 층\r\n","        # 크기변화: (배치 크기, 시퀀스 길이, 임베딩 차원)\r\n","        # => output (배치 크기, 시퀀스 길이, 은닉층 크기), hidden (1, 배치 크기, 은닉층 크기)\r\n","        output, hidden = self.rnn_layer(output)\r\n","        # 3. 최종 출력층\r\n","        # 크기변화: (배치 크기, 시퀀스 길이, 은닉층 크기) => (배치 크기, 시퀀스 길이, 단어장 크기(vocab_size)) (1, 6, 8)\r\n","        output = self.linear(output)\r\n","        # 4. view를 통해서 배치 차원 제거\r\n","        # 크기변화: (배치 크기, 시퀀스 길이, 단어장 크기) => (배치 크기*시퀀스 길이, 단어장 크기) (1 * 6, 8)\r\n","        return output.view(-1, output.size(2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQBu8p_geSNc"},"source":["# 하이퍼 파라미터\r\n","vocab_size = len(word2index)  # 단어장의 크기는 임베딩 층, 최종 출력층에 사용된다. <unk> 토큰을 크기에 포함한다.\r\n","input_size = 5  # 임베딩 된 차원의 크기 및 RNN 층 입력 차원의 크기\r\n","hidden_size = 20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cmdfYnnFh9Dh","executionInfo":{"status":"ok","timestamp":1611664308770,"user_tz":-540,"elapsed":627,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}},"outputId":"3c16088c-d822-47b7-fad6-77510f01c961"},"source":["len(word2index)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"iDu5z_mWfZGi"},"source":["# 모델 생성\r\n","model = Net(vocab_size, input_size, hidden_size, batch_first = True)\r\n","# 손실함수 정의\r\n","loss_function = nn.CrossEntropyLoss()  # 소프트맥스 함수 포함이며 실제값은 원-핫 인코딩 안 해도 됨\r\n","# 옵티마이저 정의\r\n","optimizer = optim.Adam(params = model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"agVY4RR7iaG8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611664339679,"user_tz":-540,"elapsed":743,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}},"outputId":"921ebbf1-359d-402c-abb7-642a8977cb4d"},"source":["# 임의로 예측해보기, 가중치는 전부 랜덤 초기화된 상태이다.\r\n","output = model(X)\r\n","print(output)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[-0.0022, -0.1335,  0.0503, -0.2161, -0.2643, -0.1881, -0.1972, -0.1656],\n","        [-0.1385, -0.2568, -0.0525, -0.2104, -0.4541, -0.2294,  0.1212, -0.3116],\n","        [ 0.1584, -0.3052, -0.0091, -0.2901, -0.1001, -0.2770, -0.0428, -0.1306],\n","        [ 0.0217, -0.1530, -0.0619, -0.1706, -0.3434, -0.2223, -0.1040, -0.0685],\n","        [-0.0705, -0.1156, -0.1755,  0.0048, -0.4573, -0.0704, -0.4216, -0.1748],\n","        [-0.0162, -0.1154, -0.2930,  0.0753, -0.2878, -0.0013, -0.3668,  0.0212]],\n","       grad_fn=<ViewBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DViV-c3IghJ2","executionInfo":{"status":"ok","timestamp":1611664340785,"user_tz":-540,"elapsed":480,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}},"outputId":"d80653af-57fb-4448-f4ed-c3c0a3494808"},"source":["print(output.shape)  # (배치 크기*시퀀스 길이, 단어장 크기) / (1 * 6, 8) / < unk > 까지 포함"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([6, 8])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dcx2UVcDgwNG","executionInfo":{"status":"ok","timestamp":1611664341753,"user_tz":-540,"elapsed":841,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}},"outputId":"5578d678-ef20-46d3-d22a-6d342e53c6a3"},"source":["torch.Size([6,8])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([6, 8])"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"FLcgo7Kog2sJ"},"source":["# 수치화된 데이터를 단어로 전호나하는 함수\r\n","decode = lambda y: [index2word.get(x) for x in y]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HmQxk8isg-p2","executionInfo":{"status":"ok","timestamp":1611664343148,"user_tz":-540,"elapsed":568,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}},"outputId":"ecb0330a-e7a6-46a3-a084-0a3b47710405"},"source":["decode"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function __main__.<lambda>>"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"cqvI9irzledT"},"source":["# 2. 훈련 시작"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zzHHbJTjRkkK","executionInfo":{"status":"ok","timestamp":1611664357410,"user_tz":-540,"elapsed":1198,"user":{"displayName":"홍유빈학부생","photoUrl":"","userId":"00679493735292500145"}},"outputId":"563b42ba-7da1-4954-b0fa-1bcd30e49dce"},"source":["# 훈련 시작\r\n","for step in range(201):\r\n","    # 경사 초기화\r\n","    optimizer.zero_grad()\r\n","    # 순방향 전파\r\n","    output = model(X)\r\n","    # 손실값 계산\r\n","    loss = loss_function(output, Y.view(-1))\r\n","    # 역방향 전파\r\n","    loss.backward()\r\n","    # 매개변수 업데이트\r\n","    optimizer.step()\r\n","    # 기록\r\n","    if step % 40 == 0:\r\n","        print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\r\n","        pred = output.softmax(-1).argmax(-1).tolist()\r\n","        print(\" \".join([\"Repeat\"] + decode(pred)))\r\n","        print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[01/201] 2.0391 \n","Repeat medicine best <unk> <unk> for for\n","\n","[41/201] 1.4054 \n","Repeat medicine the best medicine for memory\n","\n","[81/201] 0.7707 \n","Repeat is the best medicine for memory\n","\n","[121/201] 0.3770 \n","Repeat is the best medicine for memory\n","\n","[161/201] 0.1917 \n","Repeat is the best medicine for memory\n","\n","[201/201] 0.1120 \n","Repeat is the best medicine for memory\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kJbGODdORkmt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PaGcOmKdRkpM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQXv8jV0Rkzd"},"source":[""],"execution_count":null,"outputs":[]}]}